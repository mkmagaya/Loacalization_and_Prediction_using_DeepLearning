# -*- coding: utf-8 -*-
"""Copy of Loacalization and Prediction using DeepLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZmDYYLik16UyXH_T9romJ5G1PZPCuuTs

#  localization and locatoion prediction with deep learning

Since GPS signals can be unreliable this project uses wifi fingerprints in order to predict the location of users. Data: 19,937 training & 1111 validation datapoints with 529 attributes (wifi fingerprints) measured by 25 different android devices.Each WiFi fingerprint can be characterized by the detected Wireless Access Points (WAPs) and the corresponding Received Signal Strength Intensity (RSSI). The intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM. The positive value 100 is used to denote when a WAP was not detected. During the database creation, 520 different WAPs were detected. Thus, the WiFi fingerprint is composed by 520 intensity values.

Then the coordinates (latitude, longitude, floor) and Building ID are provided as the attributes to be predicted.
Relative positioning and latitude and longitude prediction is the first step to modelling a succesfull location based ad campaign,followed by maping the actual area where the target(mobile devise to which the advert will be shown) then choosing the appropriate ad.

> Indented block


 ## LINVAL T CHISOKO R181563R
 ## MAKOMBORERO MAGAYA R181571B
 ## SAMANTHA MUGARI R181725N

# Modules Imports
"""

''''''

# Imports

# necessary Libraries
import numpy as np
import pandas as pd
import time
import pprint

#Visualizations
import matplotlib.pyplot as plt
import seaborn as sns


#Preprocessing
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA 
from scipy.sparse import lil_matrix

#Scoring Metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error



import tensorflow as tf
from keras.models import Sequential
from keras.layers import *

# Commented out IPython magic to ensure Python compatibility.
import json
import csv
import pandas as pd
import numpy as np 
import os
from scipy.stats import zscore               # min max scaler


from sklearn.preprocessing import MinMaxScaler
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.metrics import confusion_matrix, classification_report

# %matplotlib inline
import matplotlib.pyplot as plt

from matplotlib.pyplot import figure, show
from sklearn.model_selection import train_test_split
from sklearn import metrics
import tensorflow as tf
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint

# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Plot a confusion matrix.
# cm is the confusion matrix, names are the names of the classes.
def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(names))
    plt.xticks(tick_marks, names, rotation=45)
    plt.yticks(tick_marks, names)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    

# Plot an ROC. pred - the predictions, y - the expected output.
def plot_roc(pred,y):
    fpr, tpr, thresholds = roc_curve(y, pred)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.show()
    
# imports for tf and other models

import collections
from sklearn import preprocessing
import matplotlib.pyplot as plt
import shutil

def to_xy(df, target):
    result = []
    for x in df.columns:
        if x != target:
            result.append(x)
    # find out the type of the target column. 
    target_type = df[target].dtypes
    target_type = target_type[0] if isinstance(target_type, collections.Sequence) else target_type
    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.
    if target_type in (np.int64, np.int32, np.object):
        # Classification
        dummies = pd.get_dummies(df[target])
        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)
    else:
        # Regression
        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)
    
def encode_text_index(df, name):
    le = preprocessing.LabelEncoder()
    df[name] = le.fit_transform(df[name])
    return le.classes_

def chart_regression(pred,y,sort=True):
    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})
    if sort:
        t.sort_values(by=['y'],inplace=True)
    a = plt.plot(t['y'].tolist(),label='expected')
    b = plt.plot(t['pred'].tolist(),label='prediction')
    plt.ylabel('output')
    plt.legend()
    plt.show()
    
from sklearn.metrics import r2_score

import keras
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten
from keras.optimizers import Adam
import pandas as pd
import io
import requests
import numpy as np
from sklearn import metrics
import os
import json
import csv
import pandas as pd
import sklearn.feature_extraction.text as tfidf
from sklearn.model_selection import train_test_split
from sklearn import datasets,linear_model, preprocessing,utils
from sklearn.metrics import mean_squared_error,r2_score
from scipy.stats import zscore
import numpy as np
import pickle
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn import svm
from sklearn.metrics import f1_score
import collections
from keras import optimizers
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten
from keras.optimizers import Adam
from keras import optimizers
from keras.layers import Conv1D, Conv2D, MaxPooling2D

# Mount Drive

# from google.colab import drive
# drive.mount('/content/drive')

"""# Load Data"""

# Loading raw data

# path to the dataset
train_data = pd.read_csv("drive/My Drive/kbs/trainingData.csv")

"""# Knowing the Raw Data"""

# Dataset shape
# train_data.shape

# train_data.head()

#  getting the Datatypes in the dataset
# list(train_data.dtypes)

# print(list(train_data))

feature_list = list(train_data)

#Drop unneeded data
train_data.drop(['USERID', 'PHONEID', 'TIMESTAMP'], axis = 1, inplace=True)

#Remove "NaN" value
col = train_data.columns[0:520]
for i in col:
    train_data[i].fillna(0, inplace=True)
train_data.dropna(subset=['LONGITUDE','LATITUDE', 'FLOOR', 'BUILDINGID' ], inplace=True)



#Processing "WAP" data
train_data.iloc[:, 0:520] = np.where(train_data.iloc[:, 0:520] <= 0, 
                        train_data.iloc[:, 0:520] + 105, 
                        train_data.iloc[:, 0:520] - 100)

#Processing Longtitudinal data
train_data.iloc[:, 520] = np.where(train_data.iloc[:, 520] <= 0, 
                        -train_data.iloc[:, 520], 
                        train_data.iloc[:, 520])


# train_data.describe()

min_LGT = 7300.818990
min_LAT = 4.864746e+06

train_data.iloc[:,520] = (train_data.iloc[:, 520] - min_LGT + 1)
train_data.iloc[:,521] = (train_data.iloc[:, 521] - min_LAT + 1)

# Splitting the dataset into x and y

out_data = train_data.copy()
out_data = out_data.drop(['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007', 'WAP008', 'WAP009', 'WAP010', 'WAP011', 'WAP012', 'WAP013', 'WAP014', 'WAP015', 'WAP016', 'WAP017', 'WAP018', 'WAP019', 'WAP020', 'WAP021', 'WAP022', 'WAP023', 'WAP024', 'WAP025', 'WAP026', 'WAP027', 'WAP028', 'WAP029', 'WAP030', 'WAP031', 'WAP032', 'WAP033', 'WAP034', 'WAP035', 'WAP036', 'WAP037', 'WAP038', 'WAP039', 'WAP040', 'WAP041', 'WAP042', 'WAP043', 'WAP044', 'WAP045', 'WAP046', 'WAP047', 'WAP048', 'WAP049', 'WAP050', 'WAP051', 'WAP052', 'WAP053', 'WAP054', 'WAP055', 'WAP056', 'WAP057', 'WAP058', 'WAP059', 'WAP060', 'WAP061', 'WAP062', 'WAP063', 'WAP064', 'WAP065', 'WAP066', 'WAP067', 'WAP068', 'WAP069', 'WAP070', 'WAP071', 'WAP072', 'WAP073', 'WAP074', 'WAP075', 'WAP076', 'WAP077', 'WAP078', 'WAP079', 'WAP080', 'WAP081', 'WAP082', 'WAP083', 'WAP084', 'WAP085', 'WAP086', 'WAP087', 'WAP088', 'WAP089', 'WAP090', 'WAP091', 'WAP092', 'WAP093', 'WAP094', 'WAP095', 'WAP096', 'WAP097', 'WAP098', 'WAP099', 'WAP100', 'WAP101', 'WAP102', 'WAP103', 'WAP104', 'WAP105', 'WAP106', 'WAP107', 'WAP108', 'WAP109', 'WAP110', 'WAP111', 'WAP112', 'WAP113', 'WAP114', 'WAP115', 'WAP116', 'WAP117', 'WAP118', 'WAP119', 'WAP120', 'WAP121', 'WAP122', 'WAP123', 'WAP124', 'WAP125', 'WAP126', 'WAP127', 'WAP128', 'WAP129', 'WAP130', 'WAP131', 'WAP132', 'WAP133', 'WAP134', 'WAP135', 'WAP136', 'WAP137', 'WAP138', 'WAP139', 'WAP140', 'WAP141', 'WAP142', 'WAP143', 'WAP144', 'WAP145', 'WAP146', 'WAP147', 'WAP148', 'WAP149', 'WAP150', 'WAP151', 'WAP152', 'WAP153', 'WAP154', 'WAP155', 'WAP156', 'WAP157', 'WAP158', 'WAP159', 'WAP160', 'WAP161', 'WAP162', 'WAP163', 'WAP164', 'WAP165', 'WAP166', 'WAP167', 'WAP168', 'WAP169', 'WAP170', 'WAP171', 'WAP172', 'WAP173', 'WAP174', 'WAP175', 'WAP176', 'WAP177', 'WAP178', 'WAP179', 'WAP180', 'WAP181', 'WAP182', 'WAP183', 'WAP184', 'WAP185', 'WAP186', 'WAP187', 'WAP188', 'WAP189', 'WAP190', 'WAP191', 'WAP192', 'WAP193', 'WAP194', 'WAP195', 'WAP196', 'WAP197', 'WAP198', 'WAP199', 'WAP200', 'WAP201', 'WAP202', 'WAP203', 'WAP204', 'WAP205', 'WAP206', 'WAP207', 'WAP208', 'WAP209', 'WAP210', 'WAP211', 'WAP212', 'WAP213', 'WAP214', 'WAP215', 'WAP216', 'WAP217', 'WAP218', 'WAP219', 'WAP220', 'WAP221', 'WAP222', 'WAP223', 'WAP224', 'WAP225', 'WAP226', 'WAP227', 'WAP228', 'WAP229', 'WAP230', 'WAP231', 'WAP232', 'WAP233', 'WAP234', 'WAP235', 'WAP236', 'WAP237', 'WAP238', 'WAP239', 'WAP240', 'WAP241', 'WAP242', 'WAP243', 'WAP244', 'WAP245', 'WAP246', 'WAP247', 'WAP248', 'WAP249', 'WAP250', 'WAP251', 'WAP252', 'WAP253', 'WAP254', 'WAP255', 'WAP256', 'WAP257', 'WAP258', 'WAP259', 'WAP260', 'WAP261', 'WAP262', 'WAP263', 'WAP264', 'WAP265', 'WAP266', 'WAP267', 'WAP268', 'WAP269', 'WAP270', 'WAP271', 'WAP272', 'WAP273', 'WAP274', 'WAP275', 'WAP276', 'WAP277', 'WAP278', 'WAP279', 'WAP280', 'WAP281', 'WAP282', 'WAP283', 'WAP284', 'WAP285', 'WAP286', 'WAP287', 'WAP288', 'WAP289', 'WAP290', 'WAP291', 'WAP292', 'WAP293', 'WAP294', 'WAP295', 'WAP296', 'WAP297', 'WAP298', 'WAP299', 'WAP300', 'WAP301', 'WAP302', 'WAP303', 'WAP304', 'WAP305', 'WAP306', 'WAP307', 'WAP308', 'WAP309', 'WAP310', 'WAP311', 'WAP312', 'WAP313', 'WAP314', 'WAP315', 'WAP316', 'WAP317', 'WAP318', 'WAP319', 'WAP320', 'WAP321', 'WAP322', 'WAP323', 'WAP324', 'WAP325', 'WAP326', 'WAP327', 'WAP328', 'WAP329', 'WAP330', 'WAP331', 'WAP332', 'WAP333', 'WAP334', 'WAP335', 'WAP336', 'WAP337', 'WAP338', 'WAP339', 'WAP340', 'WAP341', 'WAP342', 'WAP343', 'WAP344', 'WAP345', 'WAP346', 'WAP347', 'WAP348', 'WAP349', 'WAP350', 'WAP351', 'WAP352', 'WAP353', 'WAP354', 'WAP355', 'WAP356', 'WAP357', 'WAP358', 'WAP359', 'WAP360', 'WAP361', 'WAP362', 'WAP363', 'WAP364', 'WAP365', 'WAP366', 'WAP367', 'WAP368', 'WAP369', 'WAP370', 'WAP371', 'WAP372', 'WAP373', 'WAP374', 'WAP375', 'WAP376', 'WAP377', 'WAP378', 'WAP379', 'WAP380', 'WAP381', 'WAP382', 'WAP383', 'WAP384', 'WAP385', 'WAP386', 'WAP387', 'WAP388', 'WAP389', 'WAP390', 'WAP391', 'WAP392', 'WAP393', 'WAP394', 'WAP395', 'WAP396', 'WAP397', 'WAP398', 'WAP399', 'WAP400', 'WAP401', 'WAP402', 'WAP403', 'WAP404', 'WAP405', 'WAP406', 'WAP407', 'WAP408', 'WAP409', 'WAP410', 'WAP411', 'WAP412', 'WAP413', 'WAP414', 'WAP415', 'WAP416', 'WAP417', 'WAP418', 'WAP419', 'WAP420', 'WAP421', 'WAP422', 'WAP423', 'WAP424', 'WAP425', 'WAP426', 'WAP427', 'WAP428', 'WAP429', 'WAP430', 'WAP431', 'WAP432', 'WAP433', 'WAP434', 'WAP435', 'WAP436', 'WAP437', 'WAP438', 'WAP439', 'WAP440', 'WAP441', 'WAP442', 'WAP443', 'WAP444', 'WAP445', 'WAP446', 'WAP447', 'WAP448', 'WAP449', 'WAP450', 'WAP451', 'WAP452', 'WAP453', 'WAP454', 'WAP455', 'WAP456', 'WAP457', 'WAP458', 'WAP459', 'WAP460', 'WAP461', 'WAP462', 'WAP463', 'WAP464', 'WAP465', 'WAP466', 'WAP467', 'WAP468', 'WAP469', 'WAP470', 'WAP471', 'WAP472', 'WAP473', 'WAP474', 'WAP475', 'WAP476', 'WAP477', 'WAP478', 'WAP479', 'WAP480', 'WAP481', 'WAP482', 'WAP483', 'WAP484', 'WAP485', 'WAP486', 'WAP487', 'WAP488', 'WAP489', 'WAP490', 'WAP491', 'WAP492', 'WAP493', 'WAP494', 'WAP495', 'WAP496', 'WAP497', 'WAP498', 'WAP499', 'WAP500', 'WAP501', 'WAP502', 'WAP503', 'WAP504', 'WAP505', 'WAP506', 'WAP507', 'WAP508', 'WAP509', 'WAP510', 'WAP511', 'WAP512', 'WAP513', 'WAP514', 'WAP515', 'WAP516', 'WAP517', 'WAP518', 'WAP519', 'WAP520','SPACEID'], axis = 1)

# print(list(out_data))

in_data = train_data.copy()
in_data = in_data.drop(['LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID', 'SPACEID', 'RELATIVEPOSITION'], axis = 1)

# print(list(in_data))

in_data.shape, out_data.shape

"""# Feature Importance

"""

# Feature Importances

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier

feature_names = ['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007', 'WAP008', 'WAP009', 'WAP010', 'WAP011', 'WAP012', 'WAP013', 'WAP014', 'WAP015', 'WAP016', 'WAP017', 'WAP018', 'WAP019', 'WAP020', 'WAP021', 'WAP022', 'WAP023', 'WAP024', 'WAP025', 'WAP026', 'WAP027', 'WAP028', 'WAP029', 'WAP030', 'WAP031', 'WAP032', 'WAP033', 'WAP034', 'WAP035', 'WAP036', 'WAP037', 'WAP038', 'WAP039', 'WAP040', 'WAP041', 'WAP042', 'WAP043', 'WAP044', 'WAP045', 'WAP046', 'WAP047', 'WAP048', 'WAP049', 'WAP050', 'WAP051', 'WAP052', 'WAP053', 'WAP054', 'WAP055', 'WAP056', 'WAP057', 'WAP058', 'WAP059', 'WAP060', 'WAP061', 'WAP062', 'WAP063', 'WAP064', 'WAP065', 'WAP066', 'WAP067', 'WAP068', 'WAP069', 'WAP070', 'WAP071', 'WAP072', 'WAP073', 'WAP074', 'WAP075', 'WAP076', 'WAP077', 'WAP078', 'WAP079', 'WAP080', 'WAP081', 'WAP082', 'WAP083', 'WAP084', 'WAP085', 'WAP086', 'WAP087', 'WAP088', 'WAP089', 'WAP090', 'WAP091', 'WAP092', 'WAP093', 'WAP094', 'WAP095', 'WAP096', 'WAP097', 'WAP098', 'WAP099', 'WAP100', 'WAP101', 'WAP102', 'WAP103', 'WAP104', 'WAP105', 'WAP106', 'WAP107', 'WAP108', 'WAP109', 'WAP110', 'WAP111', 'WAP112', 'WAP113', 'WAP114', 'WAP115', 'WAP116', 'WAP117', 'WAP118', 'WAP119', 'WAP120', 'WAP121', 'WAP122', 'WAP123', 'WAP124', 'WAP125', 'WAP126', 'WAP127', 'WAP128', 'WAP129', 'WAP130', 'WAP131', 'WAP132', 'WAP133', 'WAP134', 'WAP135', 'WAP136', 'WAP137', 'WAP138', 'WAP139', 'WAP140', 'WAP141', 'WAP142', 'WAP143', 'WAP144', 'WAP145', 'WAP146', 'WAP147', 'WAP148', 'WAP149', 'WAP150', 'WAP151', 'WAP152', 'WAP153', 'WAP154', 'WAP155', 'WAP156', 'WAP157', 'WAP158', 'WAP159', 'WAP160', 'WAP161', 'WAP162', 'WAP163', 'WAP164', 'WAP165', 'WAP166', 'WAP167', 'WAP168', 'WAP169', 'WAP170', 'WAP171', 'WAP172', 'WAP173', 'WAP174', 'WAP175', 'WAP176', 'WAP177', 'WAP178', 'WAP179', 'WAP180', 'WAP181', 'WAP182', 'WAP183', 'WAP184', 'WAP185', 'WAP186', 'WAP187', 'WAP188', 'WAP189', 'WAP190', 'WAP191', 'WAP192', 'WAP193', 'WAP194', 'WAP195', 'WAP196', 'WAP197', 'WAP198', 'WAP199', 'WAP200', 'WAP201', 'WAP202', 'WAP203', 'WAP204', 'WAP205', 'WAP206', 'WAP207', 'WAP208', 'WAP209', 'WAP210', 'WAP211', 'WAP212', 'WAP213', 'WAP214', 'WAP215', 'WAP216', 'WAP217', 'WAP218', 'WAP219', 'WAP220', 'WAP221', 'WAP222', 'WAP223', 'WAP224', 'WAP225', 'WAP226', 'WAP227', 'WAP228', 'WAP229', 'WAP230', 'WAP231', 'WAP232', 'WAP233', 'WAP234', 'WAP235', 'WAP236', 'WAP237', 'WAP238', 'WAP239', 'WAP240', 'WAP241', 'WAP242', 'WAP243', 'WAP244', 'WAP245', 'WAP246', 'WAP247', 'WAP248', 'WAP249', 'WAP250', 'WAP251', 'WAP252', 'WAP253', 'WAP254', 'WAP255', 'WAP256', 'WAP257', 'WAP258', 'WAP259', 'WAP260', 'WAP261', 'WAP262', 'WAP263', 'WAP264', 'WAP265', 'WAP266', 'WAP267', 'WAP268', 'WAP269', 'WAP270', 'WAP271', 'WAP272', 'WAP273', 'WAP274', 'WAP275', 'WAP276', 'WAP277', 'WAP278', 'WAP279', 'WAP280', 'WAP281', 'WAP282', 'WAP283', 'WAP284', 'WAP285', 'WAP286', 'WAP287', 'WAP288', 'WAP289', 'WAP290', 'WAP291', 'WAP292', 'WAP293', 'WAP294', 'WAP295', 'WAP296', 'WAP297', 'WAP298', 'WAP299', 'WAP300', 'WAP301', 'WAP302', 'WAP303', 'WAP304', 'WAP305', 'WAP306', 'WAP307', 'WAP308', 'WAP309', 'WAP310', 'WAP311', 'WAP312', 'WAP313', 'WAP314', 'WAP315', 'WAP316', 'WAP317', 'WAP318', 'WAP319', 'WAP320', 'WAP321', 'WAP322', 'WAP323', 'WAP324', 'WAP325', 'WAP326', 'WAP327', 'WAP328', 'WAP329', 'WAP330', 'WAP331', 'WAP332', 'WAP333', 'WAP334', 'WAP335', 'WAP336', 'WAP337', 'WAP338', 'WAP339', 'WAP340', 'WAP341', 'WAP342', 'WAP343', 'WAP344', 'WAP345', 'WAP346', 'WAP347', 'WAP348', 'WAP349', 'WAP350', 'WAP351', 'WAP352', 'WAP353', 'WAP354', 'WAP355', 'WAP356', 'WAP357', 'WAP358', 'WAP359', 'WAP360', 'WAP361', 'WAP362', 'WAP363', 'WAP364', 'WAP365', 'WAP366', 'WAP367', 'WAP368', 'WAP369', 'WAP370', 'WAP371', 'WAP372', 'WAP373', 'WAP374', 'WAP375', 'WAP376', 'WAP377', 'WAP378', 'WAP379', 'WAP380', 'WAP381', 'WAP382', 'WAP383', 'WAP384', 'WAP385', 'WAP386', 'WAP387', 'WAP388', 'WAP389', 'WAP390', 'WAP391', 'WAP392', 'WAP393', 'WAP394', 'WAP395', 'WAP396', 'WAP397', 'WAP398', 'WAP399', 'WAP400', 'WAP401', 'WAP402', 'WAP403', 'WAP404', 'WAP405', 'WAP406', 'WAP407', 'WAP408', 'WAP409', 'WAP410', 'WAP411', 'WAP412', 'WAP413', 'WAP414', 'WAP415', 'WAP416', 'WAP417', 'WAP418', 'WAP419', 'WAP420', 'WAP421', 'WAP422', 'WAP423', 'WAP424', 'WAP425', 'WAP426', 'WAP427', 'WAP428', 'WAP429', 'WAP430', 'WAP431', 'WAP432', 'WAP433', 'WAP434', 'WAP435', 'WAP436', 'WAP437', 'WAP438', 'WAP439', 'WAP440', 'WAP441', 'WAP442', 'WAP443', 'WAP444', 'WAP445', 'WAP446', 'WAP447', 'WAP448', 'WAP449', 'WAP450', 'WAP451', 'WAP452', 'WAP453', 'WAP454', 'WAP455', 'WAP456', 'WAP457', 'WAP458', 'WAP459', 'WAP460', 'WAP461', 'WAP462', 'WAP463', 'WAP464', 'WAP465', 'WAP466', 'WAP467', 'WAP468', 'WAP469', 'WAP470', 'WAP471', 'WAP472', 'WAP473', 'WAP474', 'WAP475', 'WAP476', 'WAP477', 'WAP478', 'WAP479', 'WAP480', 'WAP481', 'WAP482', 'WAP483', 'WAP484', 'WAP485', 'WAP486', 'WAP487', 'WAP488', 'WAP489', 'WAP490', 'WAP491', 'WAP492', 'WAP493', 'WAP494', 'WAP495', 'WAP496', 'WAP497', 'WAP498', 'WAP499', 'WAP500', 'WAP501', 'WAP502', 'WAP503', 'WAP504', 'WAP505', 'WAP506', 'WAP507', 'WAP508', 'WAP509', 'WAP510', 'WAP511', 'WAP512', 'WAP513', 'WAP514', 'WAP515', 'WAP516', 'WAP517', 'WAP518', 'WAP519', 'WAP520', 'LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP']

# Building  a classification task using 3 informative features
Xe, ye = make_classification(n_samples=19937,
                           n_features=529,
                           n_informative=5,
                           n_redundant=0,
                           n_repeated=0,
                           n_classes=2,
                           random_state=0,
                           shuffle=False)

# Building a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0)
plt.rcParams["figure.figsize"] = (28,250)
forest.fit(Xe, ye)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)

# Plotting the feature importances of the forest
# plt.figure()
# plt.title("Feature importances")
# plt.barh(range(Xe.shape[1]), importances[indices],
#        color="r", xerr=std[indices], align="center")
# plt.yticks(range(Xe.shape[1]), [feature_names[i] for i in indices])
# plt.ylim([-1, Xe.shape[1]])
# plt.show()

"""Checking for Missing Values in all columns"""

# train_data.isnull().sum().sum()

"""# Normalization

Output arrays for all predictions
"""

# Output arrays for all predictions

# for Lat & Long

y_lat = out_data.copy()
y_lat = y_lat.drop(['FLOOR', 'BUILDINGID', 'RELATIVEPOSITION'], axis = 1)
y_lat = lil_matrix(y_lat).toarray()

#for Relative position 

y_rp = out_data.copy()
y_rp = y_rp.drop(['LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID',], axis = 1)
y_rp = lil_matrix(y_rp).toarray()

# Normalizing the in_data 

scaler = StandardScaler()
scaler.fit(in_data)    
X = scaler.transform(in_data)
# X.shape

# y_lat.shape,y_rp.shape

"""# Predicting Latitude and Longitude (LAT)"""

x_train, x_test, y_train, y_test = train_test_split(in_data, y_lat, test_size=0.25, random_state=45)

# x_train.shape, y_train.shape, x_test.shape, y_test.shape

"""Neural Network

Using Raw data for prediction
"""

start_time = time.time()

# Define the model
model_3 = Sequential()
model_3.add(Dense(300, input_dim=520, activation='relu'))
model_3.add(BatchNormalization())
model_3.add(Dropout(0.2))
model_3.add(Dense(300, activation='relu'))
model_3.add(BatchNormalization())
model_3.add(Dropout(0.2))
model_3.add(Dense(300, activation='relu'))
model_3.add(BatchNormalization())
model_3.add(Dense(2, activation='linear'))
model_3.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])


# model_3.summary()

monitor2 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')
hist_latlong=model_3.fit(
    x_train,
    y_train,
    batch_size=1024,
    validation_split = 0.4,
    epochs=200,
    shuffle=True,
    callbacks=[monitor2],
    verbose=2,
    validation_data=(x_test,y_test)
)

pred = model_3.predict(x_test)

pred = np.argmax(pred,axis=1) # raw probabilities to choose class (highest probability)


y_true= np.argmax(y_test,axis=1) 
sac1 = metrics.accuracy_score(y_true, pred)
# print("Accuracy score: {}".format(sac1))

# predictions3 = (model_3.predict(x_test))

# accuracy
# print("RMSE of predicting LONGTITUDE = ", mean_absolute_error(y_test[:,0],predictions3[:,0]))
# print("RMSE of predicting LATITUDE = ", mean_absolute_error(y_test[:,1],predictions3[:,1]))


# print("--- Run time: %s mins ---" % np.round(((time.time() - start_time)/60),2))

# print(hist_latlong.history.keys())

# plt.rcParams["figure.figsize"] = (6, 3)
# plt.plot(hist_latlong.history['accuracy'])
# plt.plot(hist_latlong.history['val_accuracy'])
# plt.title('Model Accuracy for predicting Latitude and Longitude')
# plt.ylabel('accuracy')
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
# plt.show()

"""*Normalized Model*"""

# Normalized
x_train, x_test, y_train, y_test = train_test_split(X, y_lat, test_size=0.3, random_state=45)

start_time = time.time()

# Define the model
model_3 = Sequential()
model_3.add(Dense(300, input_dim=520, activation='relu'))
model_3.add(BatchNormalization())
model_3.add(Dropout(0.2))
model_3.add(Dense(150, activation='relu'))
model_3.add(BatchNormalization())
model_3.add(Dropout(0.2))
model_3.add(Dense(150, activation='relu'))
model_3.add(BatchNormalization())
model_3.add(Dense(2, activation='linear'))
model_3.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])


# model_3.summary()

from keras.models import Sequential
from keras.layers import Dense
from keras.utils.vis_utils import plot_model
# plot_model(model_3, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

monitor2 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')
hist_latlong=model_3.fit(
    x_train,
    y_train,
    batch_size=1024,
    validation_split = 0.4,
    epochs=200,
    shuffle=True,
    callbacks=[monitor2],
    verbose=2,
    validation_data=(x_test,y_test)
)

pred = model_3.predict(x_test)

pred = np.argmax(pred,axis=1) # raw probabilities to choose class (highest probability)


y_true= np.argmax(y_test,axis=1) 
sac1 = metrics.accuracy_score(y_true, pred)
# print("Accuracy score: {}".format(sac1))

predictions3 = (model_3.predict(x_test))

# accuracy
# print("RMSE of predicting LONGTITUDE = ", mean_absolute_error(y_test[:,0],predictions3[:,0]))
# print("RMSE of predicting LATITUDE = ", mean_absolute_error(y_test[:,1],predictions3[:,1]))


# print("--- Run time: %s mins ---" % np.round(((time.time() - start_time)/60),2))

# print(hist_latlong.history.keys())

# plt.rcParams["figure.figsize"] = (6, 3)
# plt.plot(hist_latlong.history['accuracy'])
# plt.plot(hist_latlong.history['val_accuracy'])
# plt.title('Model Accuracy for predicting Latitude and Longitude(Normalised Model)')
# plt.ylabel('accuracy')S
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
# plt.show()

"""# Predicting Relative Position (RP)"""

x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y_rp, test_size=0.3, random_state=42)

"""### Neural Network"""

start_time = time.time()

# Define the model
model_rp = Sequential()
model_rp.add(Dense(300, input_dim=520, activation='relu'))
model_rp.add(BatchNormalization())
model_rp.add(Dropout(0.3))
model_rp.add(Dense(150, activation='relu'))
model_rp.add(BatchNormalization())
model_rp.add(Dropout(0.3))
model_rp.add(Dense(300, activation='relu'))
model_rp.add(BatchNormalization())
model_rp.add(Dense(2, activation='linear'))
model_rp.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])


# model_rp.summary()

monitor2 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=7, verbose=1, mode='auto')
hist_rp=model_rp.fit(
    x_train1,
    y_train1,
    batch_size=700,
    validation_split = 0.2,
    epochs=150,
    shuffle=True,
    callbacks=[monitor2],
    verbose=2,
    validation_data=(x_test1,y_test1)
)

pred = model_rp.predict(x_test1)

pred = np.argmax(pred,axis=1) # raw probabilities to choose class (highest probability)

y_true= np.argmax(y_test1,axis=1) 
sac1 = metrics.accuracy_score(y_true, pred)
# print("Accuracy score: {}".format(sac1))

"""
**other addtional models that may complement this basic location and localization model are**

*   a model to predict next location and calculate the distance beetween two adjacent location
*   home and work location prediction

"""